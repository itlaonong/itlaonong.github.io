---
title: 突破超星限制 手动下载书籍
date: 2018-08-15 13:14:52
tags: 杂记
---

# 原由

偶然看到edu邮箱可以申请idea的免费授权，购买苹果设备也能有优惠。就想着看看学校的邮箱还能不能进去。尝试了很多次，都是提示密码错误。百度找回密码的方法，提示学校有内网的平台。为了进入学校内网，我登上了学校的图书馆，找到之前用于连接学校图书馆数据库的VPN。发现我的账号还可以用。但是试了下，登录内网平台，提示我已离校。遂放弃，就顺便逛了逛学校的文章数据库，点开了超星图书。突发奇想，我既然可以在浏览器看，那么就可以把书籍下载下了才对。

> 超星对于网页版有限制，只能打印最多5页。想通过虚拟打印机转pdf的方式是行不通的。

![打印限制](/uploads/chaoxing/1.png)

<!-- more -->

# 方法

## 分析

打开一本书籍进行网页阅读。在阅读器界面进入开发者选项。观察network面板。以此查看都请求了哪些资源。就可以发现其中有图书每页的扫描图片版。

![network请求](/uploads/chaoxing/2.png)

![network请求](/uploads/chaoxing/3.png)

![network请求](/uploads/chaoxing/4.png)

一共有2个请求，第一个请求的返回码是302。所以其实过程应该是第一个请求被重定向到第二个地址了。可以看到第二个地址就是图片的真实地址。

```
http://img.sslibrary.com/n/37145edc647abce0ee08822ec8fabcb1MC208449122774/img0/74EE5C55A4DCB6229C5CC62EADF71C29B600D91D03F6ED0C0A5C9E842B735DA342E4CD1FEBC7D7253CD861C66F10EC3114DB8AB23C9163C72D3CA84267DD441F5CCF24903228F96C91D686A7D86AB07B2E571E0D3188930EA17B54E1623676BA4D96DBA63EF5D3E3E05BA13F2E764150B244/bf1/qw/12871863/E445DC23100B43138BEF667FC0EAA683/000001?zoom=0&_t=1803553745
```

```
http://bpng1.5read.com/image/ss2jpg.dll?did=bf1&pid=74EE5C55A4DCB6229C5CC62EADF71C29B600D91D03F6ED0C0A5C9E842B735DA342E4CD1FEBC7D7253CD861C66F10EC3114DB8AB23C9163C72D3CA84267DD441F5CCF24903228F96C91D686A7D86AB07B2E571E0D3188930EA17B54E1623676BA4D96DBA63EF5D3E3E05BA13F2E764150B244E445DC23100B43138BEF667FC0EAA683&jid=/000001.jpg&a=9A4D589488C2828890BACEA3D05652B34680860BD3CFFFF601782BEAD167E03B39004AA821925A769F67918742F5F57B854D4FA1CD926C6A5BDEDE913140362716E9&zoom=0&f=0
```

换到下一页，查看地址的规律，可以发现地址主要的不同就是在于最后面的000001和000002。（参数_t猜测是随机数，经测试确实不影响实际效果，zoom参数为清晰度，后面具体介绍）

![network请求](/uploads/chaoxing/5.png)

![network请求](/uploads/chaoxing/6.png)

```
http://img.sslibrary.com/n/37145edc647abce0ee08822ec8fabcb1MC208449122774/img0/74EE5C55A4DCB6229C5CC62EADF71C29B600D91D03F6ED0C0A5C9E842B735DA342E4CD1FEBC7D7253CD861C66F10EC3114DB8AB23C9163C72D3CA84267DD441F5CCF24903228F96C91D686A7D86AB07B2E571E0D3188930EA17B54E1623676BA4D96DBA63EF5D3E3E05BA13F2E764150B244/bf1/qw/12871863/E445DC23100B43138BEF667FC0EAA683/000002?zoom=0&_t=520730457
```

```
http://bpng1.5read.com/image/ss2jpg.dll?did=bf1&pid=74EE5C55A4DCB6229C5CC62EADF71C29B600D91D03F6ED0C0A5C9E842B735DA342E4CD1FEBC7D7253CD861C66F10EC3114DB8AB23C9163C72D3CA84267DD441F5CCF24903228F96C91D686A7D86AB07B2E571E0D3188930EA17B54E1623676BA4D96DBA63EF5D3E3E05BA13F2E764150B244E445DC23100B43138BEF667FC0EAA683&jid=/000002.jpg&a=F7A988B01265C9FDC22C0F37B335548C840C7718FD74F9274B329D2E6798572CC25958CE21A89B41F9F94AA1CF473C635BC444A631BFFA6C246CD03E925941149923&zoom=0&f=0
```

虽然请求会被重定向到图片的真是地址，但是发现真是地址没有很好的规律，尝试更改图片名称（000001.jpg改成000002.jpg）并不能成功访问，从地址对比也能看到，还有其他参数不一样。所以就直接使用第一个地址进行抓取。让软件自己重定向。

## 抓取图片

好了，现在已经知道图片的地址规律，可以写代码将整本书的所有页面都下载下了，然后使用 Adobe Acrobat 进行合并成pdf。

虽然我是java开发，但是这种抓取资源的工作还是python比较合适。

```python
import urllib.parse
import urllib.request


for num in range(1,602): 
    url = 'http://img.sslibrary.com/n/37145edc647abce0ee08822ec8fabcb1MC208449122774/img0/74EE5C55A4DCB6229C5CC62EADF71C29B600D91D03F6ED0C0A5C9E842B735DA342E4CD1FEBC7D7253CD861C66F10EC3114DB8AB23C9163C72D3CA84267DD441F5CCF24903228F96C91D686A7D86AB07B2E571E0D3188930EA17B54E1623676BA4D96DBA63EF5D3E3E05BA13F2E764150B244/bf1/qw/12871863/E445DC23100B43138BEF667FC0EAA683/000'+ ('%03d' % num)+'?zoom=2'
    print(url)
    response = urllib.request.urlopen(url)
    with open('hadoop/000'+('%03d' % num)+".png", "wb") as code:
        code.write(response.read())

```

关于上面的代码，有2点：
1、num的范围是1-601，所以是range(1,602)，这个应该算是python的语法，一开始写的是601，最后一页没有下载。
2、url最后的参数zoom=2是指定分辨率的，一共有三个0,1,2。越大越清晰。这也是因为一开始下载的图片发现看不清楚，在网页阅读器上点击放大，发现该参数会发生变化。

关于python下载，爬取数据网上有很多教程。我也是随便找的一段代码改的。

最后，在指定的下文件夹下就有所有图片了，当然这只是正文部分，需要前言和目录页的话，也是相同的方式进行下载。这里使用 Adobe Acrobat 进行合并成pdf。

![下载完成](/uploads/chaoxing/7.png)

![合并完成](/uploads/chaoxing/8.png)

# 总结

其实整个流程就是分析规律，写代码按照规律下载。将代码伪装成浏览器一样去访问资源。然后整理下载的图片。因为我不是要把人家的所有书都抓下来，就没有去想怎么把这一整个流程都用代码实现了。

# 最后

之后查看其他书籍发现，还有另外一种阅读方式-pdf阅读。其实就是每一页都是pdf而不是图片。这样的话，规律更明显。而且封面和目录什么的都是一样的规律，一次性可以下全部页面。

![pdf阅读](/uploads/chaoxing/9.png)

![pdf阅读](/uploads/chaoxing/10.png)

同样的也是请求第一个地址被重定向到真是地址。所以直接对第一个地址进行请求下载就可以了。

下载代码：

```python
import urllib.parse
import urllib.request
import random;

for num in range(1,290):
    url = 'http://117.122.222.247:8080/download/getFile?fileMark=13807305&userMark=&pages=289&time=1532069788114&enc=fc99eccfce66d3ef1778057715119009&code=e4ea8c3928aa2dbcd2b6ee37fd1164d0&cpage='+ ('%d' % num) + '&random='+('%f' % random.random())

    print(url)
    response = urllib.request.urlopen(url)
    with open('oracle/'+('%03d' % num)+".pdf", "wb") as code:
        code.write(response.read())
        code.close()
```

关于代码：
1、最后面加的random随机数，是因为连着访问，报一个python错误，大概意思就是连着访问端口冲突，之后加上这个就没再出现。有人知道原因可以交流一下。

下载完成后还是使用 Adobe Acrobat 进行合并成一个pdf。

![下载完成](/uploads/chaoxing/11.png)

![合并完成](/uploads/chaoxing/12.png)

哦，对了，希望超星的人不要看到这篇文章。
